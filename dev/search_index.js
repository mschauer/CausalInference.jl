var documenterSearchIndex = {"docs":
[{"location":"library/#Library-1","page":"Library","title":"Library","text":"","category":"section"},{"location":"library/#(Partially)-directed-acyclic-graphs-(PDAGs-and-DAGs)-1","page":"Library","title":"(Partially) directed acyclic graphs (PDAGs and DAGs)","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"has_a_path\nCausalInference.graph\nCausalInference.allpairs\nCausalInference.isclique\nCausalInference.parents\nCausalInference.children\nCausalInference.isundirected\nCausalInference.isparent\nCausalInference.neighbors_undirected\nCausalInference.isoriented\nCausalInference.orientedge!\nCausalInference.neighbors_adjacent\nCausalInference.isadjacent\nCausalInference.ischild","category":"page"},{"location":"library/#CausalInference.has_a_path","page":"Library","title":"CausalInference.has_a_path","text":"has_a_path(g::AbstractGraph, U::Vector, V::VectorOrVertex, exclude_vertices::AbstractVector = T[], nbs=Graphs.outneighbors)\n\nFind if there is a (semi-directed) path connecting U with V not passing exclude_vertices, where nbs=Graphs.outneighbors determines the direction of traversal. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.graph","page":"Library","title":"CausalInference.graph","text":"graph(E)\n\nCreate Graph from edge-list.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.allpairs","page":"Library","title":"CausalInference.allpairs","text":"allpairs(v)\n\nIterator that is equivalent to combinations(x, 2) but iterates tuples.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.isclique","page":"Library","title":"CausalInference.isclique","text":"isclique(g, nodes)\n\nReturn true if all vertices in nodes are adjacent to each other in the graph g. Chickering, \"Learning equivalence classes\" (2002). Note that a 3-clique with already two undirected edges is also a clique in the neighbor sense.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.parents","page":"Library","title":"CausalInference.parents","text":"parents(g, x)\n\nParents are vertices y such that there is a directed edge y –> x. Returns sorted array.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.children","page":"Library","title":"CausalInference.children","text":"children(g, x)\n\nChildren of x in g are vertices y such that there is a directed edge y <– x. Returns sorted array.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.isundirected","page":"Library","title":"CausalInference.isundirected","text":"isundirected(g, edge::Edge)\nisundirected(g, x, y)\n\nTest if x and y are connected by a undirected edge in the graph g.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.isparent","page":"Library","title":"CausalInference.isparent","text":"isparent(g, x, y)\n\nTest if x is a parent of y in the graph g, meaning x→y.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.neighbors_undirected","page":"Library","title":"CausalInference.neighbors_undirected","text":"neighbors_undirected(g, x)\n\nAll vertices in g connected to x by an undirected edge. Returns sorted array.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.isoriented","page":"Library","title":"CausalInference.isoriented","text":"isoriented(g, edge::Edge)\nisoriented(g, x, y)\n\nTest if x and y are connected by a directed edge in the graph g, either x←y OR x→y. Can also perform the same test given an edge.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.orientedge!","page":"Library","title":"CausalInference.orientedge!","text":"orientedge!(g, x, y)\n\nUpdate the edge x-y to x→y in the graph g.  Throws error if not x - y\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.neighbors_adjacent","page":"Library","title":"CausalInference.neighbors_adjacent","text":"neighbors_adjacent(g, x)\n\nAll vertices in g connected to x by an any edge. Returns sorted array.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.isadjacent","page":"Library","title":"CausalInference.isadjacent","text":"isadjacent(g, x, y)\n\nTest if x and y are connected by a any edge in the graph g (i.e. x –- y, x –> y, or x <– y .)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.ischild","page":"Library","title":"CausalInference.ischild","text":"ischild(g, x, y)\n\nTest if x is a child of y in the graph g, meaning x←y.\n\n\n\n\n\n","category":"function"},{"location":"library/#Causal-graphs-1","page":"Library","title":"Causal graphs","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"dsep\ncpdag\nalt_cpdag\nvskel\nhas_recanting_witness\nbackdoor_criterion\nmeek_rules!\nCausalInference.meek_rule1\nCausalInference.meek_rule2\nCausalInference.meek_rule3\nCausalInference.meek_rule4\npdag2dag!","category":"page"},{"location":"library/#CausalInference.dsep","page":"Library","title":"CausalInference.dsep","text":"dsep(g::AbstractGraph, u, v, s; verbose = false)\n\nCheck  whether u and v are d-separated given set s. Algorithm: unrolled https://arxiv.org/abs/1304.1505\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.cpdag","page":"Library","title":"CausalInference.cpdag","text":"cpdag(skel::DiGraph)\n\nComputes CPDAG from a DAG using Chickering's conversion algorithm  (equivalent to the PC algorithm with d-seperation as independence test, see pc_oracle.)\n\nReference: M. Chickering: Learning equivalence classes of Bayesian network structures. Journal of Machine Learning Research 2 (2002). M. Chickering: A Transformational Characterization of Equivalent Bayesian Network Structures. (1995).\n\nNote that the edge order defined there is already partly encoded into the representation of a DiGraph.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.alt_cpdag","page":"Library","title":"CausalInference.alt_cpdag","text":"alt_cpdag(g::DiGraph)\n\nComputes CPDAG from a DAG using a simpler adaption of Chickering's conversion algorithm without ordering all edges (equivalent to the PC algorithm with d-separation as independence test, see pc_oracle.)\n\nReference: M. Chickering: Learning equivalence classes of Bayesian network structures. Journal of Machine Learning Research 2 (2002). M. Chickering: A Transformational Characterization of Equivalent Bayesian Network Structures. (1995).\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.vskel","page":"Library","title":"CausalInference.vskel","text":"vskel(g)\n\nReduce a (P)DAG to skeleton and v-structures. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.has_recanting_witness","page":"Library","title":"CausalInference.has_recanting_witness","text":"has_recanting_witness(g::AbstractGraph, u, v,  blocked_edges::AbstractGraph) -> Bool\n\nIn a causal DAG with edges g, determine whether path-specific effect from vertex u to v with edges in blocked_edges can be can be computed uniquely from the data available to the investigator, which is the case if there is no \"recanting witness\". Essentially this means that blocked_edges could equivalently be replaced by a blocking only outgoing edges of u. \n\nSee Alvin, Shpitser, Pearl (2005): \"Identifiability of Path-Specific Effects\",  https://ftp.cs.ucla.edu/pub/stat_ser/r321-L.pdf.    \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.backdoor_criterion","page":"Library","title":"CausalInference.backdoor_criterion","text":"backdoor_criterion(g::AbstractGraph, u::Integer, v::Integer, Z; verbose = false)\n\nTest that given directed graph g, no node in Z is descendant of u and Z d-separates u from v  in the subgraph that only has the backdoors of u left (outgoing edges of u removed)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.meek_rules!","page":"Library","title":"CausalInference.meek_rules!","text":"meek_rules!(g; rule4=false)\n\nApply Meek's rules 1-3 or 1-4 with rule4=true to orient edges in a  partially directed graph without creating cycles or new v-structures. Rule 4 is needed if edges are compelled/preoriented using external knowledge.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.meek_rule1","page":"Library","title":"CausalInference.meek_rule1","text":"meek_rule1(dg, v, w)\n\nRule 1: Orient v-w into v->w whenever there is u->v such that u and w are not adjacent (otherwise a new v-structure is created.)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.meek_rule2","page":"Library","title":"CausalInference.meek_rule2","text":"meek_rule2(dg, v, w)\n\nRule 2: Orient v-w into v->w whenever there is a chain v->k->w (otherwise a directed cycle is created.)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.meek_rule3","page":"Library","title":"CausalInference.meek_rule3","text":"meek_rule3(dg, v, w)\n\nRule 3 (Diagonal): Orient v-w into v->w whenever there are two chains v-k->w and v-l->w such that k and l are nonadjacent (otherwise a new v-structure or a directed cycle is created.)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.meek_rule4","page":"Library","title":"CausalInference.meek_rule4","text":"meek_rule4(dg, v, w)\n\nRule 4: Orient v-w into v→w if v-k→l→w where adj(v,l) and not adj(k,w) [check].\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.pdag2dag!","page":"Library","title":"CausalInference.pdag2dag!","text":"pdag2dag!(g, rule4=false)\n\nComplete PDAG to DAG using meek_rules.\n\n\n\n\n\n","category":"function"},{"location":"library/#PC-algorithm-1","page":"Library","title":"PC algorithm","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"pcalg\norient_unshielded\napply_pc_rules\nskeleton\ndseporacle\nunshielded\norientable_unshielded\nplot_pc_graph_tikz\nplot_pc_graph_recipes\nplot_pc_graph_text\nCausalInference.prepare_pc_graph","category":"page"},{"location":"library/#CausalInference.pcalg","page":"Library","title":"CausalInference.pcalg","text":"pcalg(n::V, I, par...)\npcalg(g, I, par...)\n\nPerform the PC algorithm for a set of 1:n variables using the tests\n\nI(u, v, [s1, ..., sn], par...)\n\nReturns the CPDAG as DiGraph.   \n\n\n\n\n\npcalg(t, p::Float64, test::typeof(gausscitest); kwargs...)\n\nRun PC algorithm for tabular input data t using a p-value p to test for  conditional independeces using Fisher's z-transformation.\n\n\n\n\n\npcalg(t::T, p::Float64; cmitest::typeof(cmitest); kwargs...) where{T}\n\nRun PC algorithm for tabular input data t using a p-value p to detect  conditional independeces using a conditional mutual information permutation test.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.orient_unshielded","page":"Library","title":"CausalInference.orient_unshielded","text":"orient_unshielded(g, dg, S)\n\nOrient unshielded triples using the separating sets. g is an undirected graph containing edges of unknown direction, dg is an directed graph containing edges of known direction and  both v=>w and w=>vif the direction of Edge(v,w)is unknown.S` are the separating sets of edges.\n\nReturns g, dg.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.apply_pc_rules","page":"Library","title":"CausalInference.apply_pc_rules","text":"apply_pc_rules(g, dg)\n\ng is an undirected graph containing edges of unknown direction, dg is an directed graph containing edges of known direction and  both v=>w and w=>vif the direction of Edge(v,w)` is unknown.\n\nReturns the CPDAG as DiGraph. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.skeleton","page":"Library","title":"CausalInference.skeleton","text":"skeleton(n::Integer, I) -> g, S\nskeleton(g, I) -> g, S\n\nPerform the undirected PC skeleton algorithm for a set of 1:n variables using the test I. Start with a subgraph g or the complete undirected graph on n vertices. Returns skeleton graph and separating set.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.dseporacle","page":"Library","title":"CausalInference.dseporacle","text":"dseporacle(i, j, s, g)\n\nOracle for the skeleton and pcalg functions using dsep on the true graph g\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.unshielded","page":"Library","title":"CausalInference.unshielded","text":"unshielded(g)\n\nFind the unshielded triples in the cyclefree skeleton. Triples are connected vertices v-w-z where z is not a neighbour of v. Uses that edges iterates in lexicographical order.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.orientable_unshielded","page":"Library","title":"CausalInference.orientable_unshielded","text":"orientable_unshielded(g, S)\n\nFind the orientable unshielded triples in the skeleton. Triples are connected vertices v-w-z where z is not a neighbour of v. Uses that edges iterates in lexicographical order.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.plot_pc_graph_tikz","page":"Library","title":"CausalInference.plot_pc_graph_tikz","text":"CausalInference.plot_pc_graph_tikz(g, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPlot the output of the PC algorithm (TikzGraphs backend).\n\nRequires TikzGraphs to be imported\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.plot_pc_graph_recipes","page":"Library","title":"CausalInference.plot_pc_graph_recipes","text":"plot_pc_graph_recipes(g, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPlot the output of the PC algorithm (GraphRecipes backend).\n\nRequires GraphRecipes and Plots to be imported\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.plot_pc_graph_text","page":"Library","title":"CausalInference.plot_pc_graph_text","text":"plot_pc_graph_text(g::AbstractGraph, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPlot the output of the PC algorithm (text-based output).\n\nSee also: plot_pc_graph and plot_pc_graph_tikz (for TikzGraphs.jl-based plotting), plot_pc_graph_recipes (for GraphRecipes.jl-based plotting)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.prepare_pc_graph","page":"Library","title":"CausalInference.prepare_pc_graph","text":"prepare_pc_graph(g::AbstractGraph, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPrepare resulting graph for plotting with various backends.\n\n\n\n\n\n","category":"function"},{"location":"library/#Statistics-1","page":"Library","title":"Statistics","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"gausscitest\npartialcor\ncmitest","category":"page"},{"location":"library/#CausalInference.gausscitest","page":"Library","title":"CausalInference.gausscitest","text":"gausscitest(i, j, s, (C,n), c)\n\nTest for conditional independence of variable no i and j given variables in s with Gaussian test at the critical value c. C is covariance of n observations.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.partialcor","page":"Library","title":"CausalInference.partialcor","text":"partialcor(i, j, s, C)\n\nCompute the partial correlation of nodes i and j given list of nodes s using the correlation matrix C.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.cmitest","page":"Library","title":"CausalInference.cmitest","text":"cmitest(i,j,s,data,crit; kwargs...)\n\nTest for conditional independence of variables i and j given variables in s with permutation test using nearest neighbor conditional mutual information estimates at p-value crit.\n\nkeyword arguments: kwargs...: keyword arguments passed to independence tests\n\n\n\n\n\n","category":"function"},{"location":"library/#KL-Entropy-Estimators-1","page":"Library","title":"KL Entropy Estimators","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"n_ball\nkl_entropy\nkl_renyi\nkl_mutual_information\nkl_cond_mi\nkl_perm_mi_test\nkl_perm_cond_mi_test","category":"page"},{"location":"library/#CausalInference.n_ball","page":"Library","title":"CausalInference.n_ball","text":"n_ball(n::Number)\n\nComputes the volume of a n-dimensional unit sphere.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.kl_entropy","page":"Library","title":"CausalInference.kl_entropy","text":"kl_entropy(data::Array{Float64, 2}; k=5)\n\nCompute the nearest-neighbor estimate of the differential entropy of data.\n\ndata is a 2d array, with every column representing one data point.  For further information, see\n\n\"A class of Rényi information estimators for multidimensional densities\" Nikolai Leonenko, Luc Pronzato, and Vippal Savani The Annals of Statistics, 2008 https://projecteuclid.org/euclid.aos/1223908088\n\nkeyword arguments: k=5: number of nearest neighbors\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.kl_renyi","page":"Library","title":"CausalInference.kl_renyi","text":"kl_renyi(data::Array{Float64, 2}, q; k=5)\n\nCompute the nearest-neighbor estimate of the Renyi-alpha entropy of data.\n\ndata is a 2d array, with every column representing one data point.  For further information, see\n\n\"A class of Rényi information estimators for multidimensional densities\" Nikolai Leonenko, Luc Pronzato, and Vippal Savani The Annals of Statistics, 2008 https://projecteuclid.org/euclid.aos/1223908088\n\nkeyword arguments: k=5: number of nearest neighbors\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.kl_mutual_information","page":"Library","title":"CausalInference.kl_mutual_information","text":"kl_mutual_information(x, y; k=5, bias_correction=true)\n\ncompute the nearest-neighbor 'KGS' estimate of the mutual information between x and y.\n\nx and y are 2d arrays, with every column representing one data point.  For further information, see\n\n\"Estimating Mutual Information\" Alexander Kraskov, Harald Stoegbauer, and Peter Grassberger Physical Review E https://arxiv.org/pdf/cond-mat/0305641.pdf\n\n\"Demystifying Fixed k-Nearest Neighbor Information Estimators\" Weihao Gao, Sewoong Oh, Pramod Viswanath EEE International Symposium on Information Theory - Proceedings https://arxiv.org/pdf/1604.03006.pdf\n\nkeyword arguments: k=5: number of nearest neighbors bias_correction=true: flag to apply Gao's bias correction\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.kl_cond_mi","page":"Library","title":"CausalInference.kl_cond_mi","text":"kl_cond_mi(x, y, z; k=5, bias_correction=true)\n\ncompute the nearest-neighbor 'KGS' estimate of the conditional mutual information between x and y given z.\n\nx, y, and z are 2d arrays, with every column representing one data point.  keyword arguments: k=5: number of nearest neighbors bias_correction=true: flag to apply Gao's bias correction\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.kl_perm_mi_test","page":"Library","title":"CausalInference.kl_perm_mi_test","text":"kl_perm_mi_test(x, y; k=5, B=100, bias_correction=true)\n\ncompute permutation test of independence of x and y.\n\nkeyword arguments: k=5: number of nearest neighbors to use for mutual information estimate B=100: number of permutations bias_correction=true: flag to apply Gao's bias correction\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.kl_perm_cond_mi_test","page":"Library","title":"CausalInference.kl_perm_cond_mi_test","text":"kl_perm_cond_mi_test(x, y, z; k=5, B=100, kp=5, bias_correction=true)\n\ncompute permutation test of conditional independence of x and y given z.\n\nFor further information, see: \"Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information\" Jakob Runge Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain. http://proceedings.mlr.press/v84/runge18a/runge18a.pdf\n\nkeyword arguments: k=5: number of nearest neighbors to use for mutual information estimate B=100: number of permutations bias_correction=true: flag to apply Gao's bias correction\n\n\n\n\n\n","category":"function"},{"location":"library/#FCI-algorithm-1","page":"Library","title":"FCI algorithm","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"has_marks\nset_marks!\nis_collider\nis_parent\nis_triangle\nis_discriminating_path\nis_uncovered_circle_path\nis_uncovered_PD_path\nfcialg\nplot_fci_graph_tikz\nplot_fci_graph_recipes\nplot_fci_graph_text\nCausalInference.prepare_fci_graph","category":"page"},{"location":"library/#CausalInference.has_marks","page":"Library","title":"CausalInference.has_marks","text":"has_marks(dg, v1, v2, t::Tuple{Symbol, Symbol}\n\nTest if the edge between node v1 and v2 has the edge markers given by the tuple t (use the arrow macro to simplify use)\n\nExample: has_marks(dg, 1, 2, arrow\"o->\")\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.set_marks!","page":"Library","title":"CausalInference.set_marks!","text":"set_marks!(dg, v1, v2, t::Tuple{Symbol, Symbol})\n\nSet edge marks between node v1 and v2.\n\nExample: set_marks!(dg, 1, 2, arrow\"*->\")\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.is_collider","page":"Library","title":"CausalInference.is_collider","text":"is_collider(dg, v1, v2, v3)\n\nCheck if egde v1, v2 and v3 form a collider.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.is_parent","page":"Library","title":"CausalInference.is_parent","text":"is_parent(dg, v1, v2)\n\nCheck if v1 is a parent of v2.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.is_triangle","page":"Library","title":"CausalInference.is_triangle","text":"is_triangle(dg, v1, v2, v3)\n\nCheck if v1, v2 and v3 form a triangle.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.is_discriminating_path","page":"Library","title":"CausalInference.is_discriminating_path","text":"is_discriminating_path(dg, path)\n\nCheck if path is a discriminating path.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.is_uncovered_circle_path","page":"Library","title":"CausalInference.is_uncovered_circle_path","text":"is_uncovered_circle_path(dg, path)\n\nCheck if path is an uncovered circle path.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.is_uncovered_PD_path","page":"Library","title":"CausalInference.is_uncovered_PD_path","text":"is_uncovered_PD_path(dg, path)\n\nCheck if path is an uncovered potentially directed path.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.fcialg","page":"Library","title":"CausalInference.fcialg","text":"fcialg(n::V, I, par...; augmented=true, verbose=false, kwargs...)\n\nPerform the FCI algorithm for a set of n variables using the test\n\nI(u, v, [s1, ..., sn], par...)\n\nReturns the PAG as a MetaDiGraph\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.plot_fci_graph_tikz","page":"Library","title":"CausalInference.plot_fci_graph_tikz","text":"plot_fci_graph_tikz(g, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPlot the output of the FCI algorithm (TikzGraphs backend).\n\nRequires TikzGraphs to be imported\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.plot_fci_graph_recipes","page":"Library","title":"CausalInference.plot_fci_graph_recipes","text":"plot_fci_graph_recipes(g, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPlot the output of the FCI algorithm (GraphRecipes backend).\n\nRequires GraphRecipes and Plots to be imported\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.plot_fci_graph_text","page":"Library","title":"CausalInference.plot_fci_graph_text","text":"plot_fci_graph_text(g::AbstractGraph, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPlot the output of the FCI algorithm (text-based output).\n\nSee also: plot_fci_graph and plot_fci_graph_tikz (for TikzGraphs.jl-based plotting), plot_fci_graph_recipes (for GraphRecipes.jl-based plotting)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.prepare_fci_graph","page":"Library","title":"CausalInference.prepare_fci_graph","text":"prepare_fci_graph(g::AbstractGraph, node_labels::AbstractVector{<:AbstractString}=String[])\n\nPrepare the output of the FCI algorithm for plotting with various backends.\n\n\n\n\n\n","category":"function"},{"location":"library/#Miscellaneous-1","page":"Library","title":"Miscellaneous","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"digraph\nvpairs\npc_oracle\nskel_oracle\nranddag\nCausalInference.disjoint_sorted\nCausalInference.ordered_edges\nCausalInference.insorted\nCausalInference.removesorted!\ngraph_to_text\nCausalInference.combinations_without","category":"page"},{"location":"library/#CausalInference.digraph","page":"Library","title":"CausalInference.digraph","text":"digraph(E)\n\nCreate DiGraph from edge-list.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.vpairs","page":"Library","title":"CausalInference.vpairs","text":"vpairs(g)\n\nReturn the edge-list as Pairs.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.pc_oracle","page":"Library","title":"CausalInference.pc_oracle","text":"pc_oracle(g)\n\nCompute CPDAG using the PC algorithm using the dseporacle on the DAG g. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.skel_oracle","page":"Library","title":"CausalInference.skel_oracle","text":"skel_oracle(g)\n\nCompute the skeleton using the dseporacle for the DAG g.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.randdag","page":"Library","title":"CausalInference.randdag","text":"randdag(n, alpha = 0.1)\n\nCreate Erdős–Rényi random DAG from randomly permuted random triangular matrix with edge probability alpha.\n\n\n\n\n\nranddag(n, k, true)\n\nCreate random DAG from randomly permuted random triangular matrix with expected degree k.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.disjoint_sorted","page":"Library","title":"CausalInference.disjoint_sorted","text":"disjoint_sorted(u, v)\n\nCheck if the intersection of sorted collections is empty. The intersection of empty collectios is empty.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.ordered_edges","page":"Library","title":"CausalInference.ordered_edges","text":"ordered_edges(dag)\n\nIterator of edges of a dag, ordered in Chickering order:\n\nPerform a topological sort on the NODES\nwhile there are unordered EDGES in g\n    Let y be the lowest ordered NODE that has an unordered EDGE incident into it\n    Let x be the highest ordered NODE for which x => y is not ordered\n    return x => y \nend\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.insorted","page":"Library","title":"CausalInference.insorted","text":"insorted(a, x)\n\nCheck if x is in the sorted collection a\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.removesorted!","page":"Library","title":"CausalInference.removesorted!","text":"removesorted(collection, item) -> contains(collection, item)\n\nRemove item from sorted collection.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.graph_to_text","page":"Library","title":"CausalInference.graph_to_text","text":"graph_to_text(g::AbstractGraph, node_labels::AbstractVector{<:AbstractString}=[]; edge_styles::AbstractDict=Dict())\n\nPrint out a graph g as a table of edges labeled by node_labels.\n\nArguments\n\ng::AbstractGraph: a graph to print\nnode_labels::AbstractVector{<:AbstractString}=[]: labels for nodes (same order as indices of nodes in g)\nedge_styles::AbstractDict=Dict(): dictionary of edge styles (e.g. Dict((1, 2) => \"->\", (2, 3) => \"<->\"))\n\nExample\n\ng = DiGraph(4)\nfor (i, j) in [(1, 2), (2, 3), (2, 4)]\n    add_edge!(g, i, j)\nend\ngraph_to_text(g)\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.combinations_without","page":"Library","title":"CausalInference.combinations_without","text":"combinations_without(a, n::Integer, w)\n\nGenerate all combinations of n elements from an indexable object except those with index w. Note that the combinations are modified inplace.\n\n\n\n\n\n","category":"function"},{"location":"library/#Adjustment-1","page":"Library","title":"Adjustment","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"ancestors\ndescendants\nalt_test_dsep\ntest_covariate_adjustment\nalt_test_backdoor\nfind_dsep\nfind_min_dsep\nfind_covariate_adjustment\nfind_backdoor_adjustment\nfind_frontdoor_adjustment\nfind_min_covariate_adjustment\nfind_min_backdoor_adjustment\nfind_min_frontdoor_adjustment\nlist_dseps\nlist_covariate_adjustment\nlist_backdoor_adjustment\nlist_frontdoor_adjustment","category":"page"},{"location":"library/#CausalInference.ancestors","page":"Library","title":"CausalInference.ancestors","text":"ancestors(g, X, veto = no_veto)\n\nReturn the set of ancestors of the set of vertices X in graph g. \n\nEvery vertex is an ancestor of itself.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.descendants","page":"Library","title":"CausalInference.descendants","text":"descendants(g, X, veto = no_veto)\n\nReturn the set of descendants of the set of vertices X in graph g. \n\nEvery vertex is a descendant of itself.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.alt_test_dsep","page":"Library","title":"CausalInference.alt_test_dsep","text":"alt_test_dsep(g, X, Y, S, veto = no_veto)\n\nCheck if sets of vertices X and Y are d-separated in g given S. \n\nAn alternative to the test_dsep function, which uses gensearch under the hood. Might be (a bit) slower. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.test_covariate_adjustment","page":"Library","title":"CausalInference.test_covariate_adjustment","text":"test_covariate_adjustment(g, X, Y, S)\n\nCheck if S is a covariate adjustment set relative to (X, Y) in graph g. \n\nBased on the sound and complete graphical criterion for covariate adjustment given in https://arxiv.org/abs/1203.3515 using the algorithmic approach proposed in https://arxiv.org/abs/1803.00116. Output is a boolean.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.alt_test_backdoor","page":"Library","title":"CausalInference.alt_test_backdoor","text":"alt_test_backdoor(g, X, Y, S)\n\nCheck if S satisfies the backdoor criterion relative to (X, Y) in graph g. \n\nThe generalization to sets X and Y differs from, e.g., Pearl (2009). See the Example section (TODO: ref).\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_dsep","page":"Library","title":"CausalInference.find_dsep","text":"find_dsep(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y), veto = no_veto)\n\nFind a d-separator Z with I  Z  R for sets of vertices X and Y in g, else return false. \n\nBased on the algorithmic approach proposed in https://arxiv.org/abs/1803.00116. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_min_dsep","page":"Library","title":"CausalInference.find_min_dsep","text":"find_min_dsep(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y), veto = no_veto)\n\nFind an inclusion minimal d-separator Z with I  Z  R for sets of vertices X and Y in g, i.e., one for which no subset is a d-separator, else return false.\n\nBased on the algorithmic approach proposed in http://auai.org/uai2019/proceedings/papers/222.pdf. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_covariate_adjustment","page":"Library","title":"CausalInference.find_covariate_adjustment","text":"find_covariate_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nFind a covariate adjustment set Z with I  Z  R for sets of vertices X and Y in g, else return false. \n\nBased on the algorithmic approach proposed in https://arxiv.org/abs/1803.00116. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_backdoor_adjustment","page":"Library","title":"CausalInference.find_backdoor_adjustment","text":"find_backdoor_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nFind a backdoor adjustment set Z with I  Z  R for sets of vertices X and Y in g, else return false. \n\nThe generalization to sets X and Y differs from, e.g., Pearl (2009). See the Example section (TODO: ref).\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_frontdoor_adjustment","page":"Library","title":"CausalInference.find_frontdoor_adjustment","text":"find_frontdoor_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nFind a frontdoor adjustment set Z with I  Z  R for sets of vertices X and Y in g, else return false. \n\nBased on the algorithm given in  https://arxiv.org/abs/2211.16468. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_min_covariate_adjustment","page":"Library","title":"CausalInference.find_min_covariate_adjustment","text":"find_min_covariate_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nFind an inclusion minimal covariate adjustment set Z with I  Z  R for sets of vertices X and Y in g, else return false. \n\nBased on the algorithmic approach proposed in https://arxiv.org/abs/1803.00116.  \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_min_backdoor_adjustment","page":"Library","title":"CausalInference.find_min_backdoor_adjustment","text":"find_min_backdoor_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nFind an inclusion minimal backdoor adjustment set Z with I  Z  R for sets of vertices X and Y in g, else return false. \n\nThe generalization to sets X and Y differs from, e.g., Pearl (2009). See the Example section (TODO: ref).\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.find_min_frontdoor_adjustment","page":"Library","title":"CausalInference.find_min_frontdoor_adjustment","text":"find_min_frontdoor_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nFind an inclusion minimal frontdoor adjustment set Z with I  Z  R for sets of vertices X and Y in g, else returns false. \n\nBased on the algorithm given in https://arxiv.org/abs/2211.16468. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.list_dseps","page":"Library","title":"CausalInference.list_dseps","text":"list_dseps(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nList all d-separators Z with I  Z  R for sets of vertices X and Y in g. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.list_covariate_adjustment","page":"Library","title":"CausalInference.list_covariate_adjustment","text":"list_covariate_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nList all covariate adjustment sets Z with I  Z  R for sets of vertices X and Y in g. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.list_backdoor_adjustment","page":"Library","title":"CausalInference.list_backdoor_adjustment","text":"list_backdoor_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nList all back-door adjustment sets Z with I  Z  R for sets of vertices X and Y in g. \n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.list_frontdoor_adjustment","page":"Library","title":"CausalInference.list_frontdoor_adjustment","text":"list_frontdoor_adjustment(g, X, Y, I = Set{eltype(g)}(), R = setdiff(Set(vertices(g)), X, Y))\n\nList all front-door adjustment sets Z with I  Z  R for sets of vertices X and Y in g. \n\n\n\n\n\n","category":"function"},{"location":"library/#GES-1","page":"Library","title":"GES","text":"","category":"section"},{"location":"library/#","page":"Library","title":"Library","text":"ges\nCausalInference.local_score\nCausalInference.Insert!\nCausalInference.Delete!","category":"page"},{"location":"library/#CausalInference.ges","page":"Library","title":"CausalInference.ges","text":"ges(data; verbose=false)\n\nCompute a causal graph for the given observed data using GES.\n\n\n\n\n\nges(n, data; score=0.0, parallel=false, verbose=false)\n\nEstimate a causal graph for the given observed data using GES.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.local_score","page":"Library","title":"CausalInference.local_score","text":"score(os::GaussianScore, p, v)\n\nLocal Gaussian BIC score.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.Insert!","page":"Library","title":"CausalInference.Insert!","text":"Insert!(g, x, y, T)\n\nInserts x->y and directs previously undirected edges t->y, t ∈ T. Here x and y are not adjacent and T are undirected-neighbors of y  that are not adjacent to x.\n\n\n\n\n\n","category":"function"},{"location":"library/#CausalInference.Delete!","page":"Library","title":"CausalInference.Delete!","text":"Delete!(g, x, y, H)\n\nDeletes x-y or x->y and directs previously undirected edges x->h and y->h for h in H.\n\n\n\n\n\n","category":"function"},{"location":"examples/backdoor_example/#Reasoning-about-experiments-1","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"","category":"section"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"The following examples is discussed as Example 3.4 in Chapter 3-4 (http://bayes.cs.ucla.edu/BOOK-2K/ch3-3.pdf). See also http://causality.cs.ucla.edu/blog/index.php/category/back-door-criterion/.","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"The causal model we are going to study can be represented using the following DAG concerning a set of variables numbered 1 to 8:","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"(Image: Example DAG)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"using CausalInference\nusing TikzGraphs\n# If you have problems with TikzGraphs.jl,\n# try alternatively plotting backend GraphRecipes.jl + Plots.jl\n# and corresponding plotting function `plot_pc_graph_recipes`\n\n\ndag = digraph([\n    1 => 3\n    3 => 6\n    2 => 5\n    5 => 8\n    6 => 7\n    7 => 8\n    1 => 4\n    2 => 4\n    4 => 6\n    4 => 8])\nt = plot_pc_graph_tikz(dag)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"We are interested in the average causal effect (ACE) of a treatment X (variable nr. 6) on an outcome Y (variable nr.  8), which stands for the expected increase of Y per unit of a controlled increase in X. Variables nr. 1 and nr. 2 are unobserved.","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"Regressing Y (nr. 8) on X (nr. 6) will fail to measure the effect because of the presence of a confounder C (variable nr.  4), which opens a backdoor path, a connection between X and Y via the path 6 ← 4 → 8 which is not causal. ","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"(Image: Confounder)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"We can avert such problems by checking the backdoor criterion. Indeed","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"backdoor_criterion(dag, 6, 8) # false","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"reports a problem.","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"One might want to condition on the confounder C (nr. 4)` to obtain the causal effect, but then  ","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"backdoor_criterion(dag, 6, 8, [4]) # still false","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"there is still a problem (  because that conditioning opens up a non-causal path via 6 ← 3 ← 1 → 4 ← 2 → 5 → 8.)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"(Image: Opened new path)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"But conditioning on both Z = [3, 4] solves the problem, as verified by the backdoor criterion.","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"backdoor_criterion(dag, 6, 8, [3, 4]) # true\nbackdoor_criterion(dag, 6, 8, [4, 5]) # true","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"(also conditioning on Z = [4, 5] would be possible.)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"Thus, regressing Y on X and controlling for variables numbered Z=[3, 4] we measure the average causal effect.","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"(Image: Good controls)","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"What we have done by hand here is the search for an backdoor adjustment set. We could have directly queried","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"Zs = list_covariate_adjustment(dag, 6, 8, Int[], setdiff(Set(1:8), [1, 2])) # exclude variables nr. 1 and nr. 2 because they are unobserved.","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"which lists possible adjustment sets,","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"println.(Zs);","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"    Set([4, 3])\n    Set([5, 4])\n    Set([5, 4, 3])","category":"page"},{"location":"examples/backdoor_example/#","page":"Reasoning about experiments","title":"Reasoning about experiments","text":"to get the list of possible adjustment sets. Here list_backdoor_adjustment gives adjustment sets for a backdoor criterion similar to backdoor_criterion and list_covariate_adjustment the more versatile adjustment set  based on the sound and complete graphical criterion for covariate adjustment given in [https://arxiv.org/abs/1203.3515] using the algorithmic approach proposed in [https://arxiv.org/abs/1803.00116]. ","category":"page"},{"location":"examples/pc_cmi_examples/#PC-Algorithm:-Further-examples-1","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"","category":"section"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"In the previous examples, we used a basic test data set to illustrate how to use the PC algorithm and how to visualise and interpret the results. The test data set was created using a simple linear model that followed the structure of the test DAG shown above. Depending on the field or discipline we are working in, we may encounter data sets whose components are not necessarily linearly related. It is hence worthwhile to analyse how the PC algorithm performs when working with more complex, nonlinear data sets:","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"using Distributions\nusing CausalInference\nusing TikzGraphs\n# If you have problems with TikzGraphs.jl, \n# try alternatively plotting backend GraphRecipes.jl + Plots.jl\n# and corresponding plotting function `plot_pc_graph_recipes`\n\nN = 1000 # number of data points\n\nx = rand(Uniform(0,2π), N)\nv = sin.(x) + randn(N)*0.25\nw = cos.(x) + randn(N)*0.25\nz = 3 * v.^2 - w + randn(N)*0.25 \ns = z.^2 + randn(N)*0.25\n\ndf = (x=x,v=v,w=w,z=z,s=s)","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"The data set created above exhibits the same causal structure, but now the individual variables are related to each other in a (deliberately) nonlinear fashion. As before, we can run the PC algorithm on this data set using the Gaussian conditional independence test:","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"plot_pc_graph_tikz(pcalg(df, 0.01, gausscitest), [String(k) for k in keys(df)])","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"(Image: PC algorithm on nonlinear data with gaussci)","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"Interestingly enough, the resulting graph bears no resemblance to the true graph we would expect to see. To understand what's happening here, we need to remind ourselves that the PC algorithm uses conditional independence tests to determine the causal relationships between different parts of our data set. It seems as if the Gaussian conditional independence test we are using here can't quite handle the nonlinear relationships present in the data (note that increasing the number of data points won't change this!).  To handle cases like this, we implemented additional conditional independence tests based on the concept of conditional mutual information (CMI). These tests have the benefit of being independent of the type of relationship between different parts of the analysed data. Using these tests instead of the Gaussian test we used previously is achieved by simply passing cmitest instead of gaussci as the conditional independence test to use to the PC algorithm:","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"plot_pc_graph_tikz(pcalg(df, 0.01, cmitest), [String(k) for k in keys(df)])","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"(Image: PC algorithm on nonlinear data with cmitest)","category":"page"},{"location":"examples/pc_cmi_examples/#","page":"PC Algorithm: Further examples","title":"PC Algorithm: Further examples","text":"The result of the PC algorithm using the CMI test again look like what we'd expect to see.  It should be pointed out here that using cmitest is significantly more computationally demanding and takes a lot longer than using gausscitest.","category":"page"},{"location":"examples/pc_real_example/#PC-Algorithm:-Example-using-real-data-1","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"","category":"section"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"To illustrate how to work with the PC algorithm on real data instead of synthetic test data, we are going to look at the Retention data set containing information on college graduation that was used in the article \"What Do College Ranking Data Tell Us About Student Retention?\" by Drudzel and Glymour (disclaimer: the idea to use this data set was taken from the causal-cmd homepage, which readers of this documentation are strongly encouraged to look at!).","category":"page"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"The can be loaded into a dataframe in a few lines of code:","category":"page"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"using HTTP, CSV, DataFrames\nusing CausalInference\nusing TikzGraphs\n# If you have problems with TikzGraphs.jl, \n# try alternatively plotting backend GraphRecipes.jl + Plots.jl\n# and corresponding plotting function `plot_pc_graph_recipes`\n\nurl = \"https://www.ccd.pitt.edu//wp-content/uploads/files/Retention.txt\"\n\ndf = DataFrame(CSV.File(HTTP.get(url).body))\n\n# for now, pcalg and fcialg only accepts Float variables...\n# this should change soon hopefully\nfor name in names(df)\n\tdf[!, name] = convert(Array{Float64,1}, df[!,name])\nend\n\n# make variable names a bit easier to read\nvariables = map(x->replace(x,\"_\"=>\" \"), names(df))","category":"page"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"This dataframe can be used as input to the PC algorithm just like the dataframes holding synthetic data before:","category":"page"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"plot_pc_graph_tikz(pcalg(df, 0.025, gausscitest), variables)","category":"page"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"(Image: Plot of PC output for true data)","category":"page"},{"location":"examples/pc_real_example/#","page":"PC Algorithm: Example using real data","title":"PC Algorithm: Example using real data","text":"At first glance, the output of the PC algorithm looks sensible and appears to provide some first insight into the data we are working with. In particular, the causal factors influencing faculty pay appear rather sensible. However, before drawing any further conclusions, different versions of the PC algorithm (e.g., different p-values, different independence tests, etc.) should be explored.","category":"page"},{"location":"comments/#Developer-comments-1","page":"Developer comments","title":"Developer comments","text":"","category":"section"},{"location":"comments/#Whether-to-use-Meek's-or-Chickerings-completion-algorithm.-1","page":"Developer comments","title":"Whether to use Meek's or Chickerings completion algorithm.","text":"","category":"section"},{"location":"comments/#","page":"Developer comments","title":"Developer comments","text":"Meek's approach we currently use in PC and GES is: Take the PDAG and apply R1-R4 repeatedly until no rule applies. To obtain the CPDAG from a pattern the rules R1-R3 suffice, R4 is not needed.","category":"page"},{"location":"comments/#","page":"Developer comments","title":"Developer comments","text":"Chickering's approach is to take the PDAG and find a DAG with same skeleton and v-structures. (This can be done e.g. with this algorithm by Dor and Tarsi: https://ftp.cs.ucla.edu/pub/stat_ser/r185-dor-tarsi.pdf) From the DAG construct the CPDAG (which is possible in linear-time, we also have this implemented cpdag(g)).","category":"page"},{"location":"comments/#","page":"Developer comments","title":"Developer comments","text":"Implementing Chickerings approach would be an option because the complicated part (from an implementation perspective) is DAG-to-CPDAG which we already have. A straightforward implementation of PDAG to DAG by Dor and Tarsi are just a few lines. For speed of PC and GES this will likely not matter too much overall, other parts are more costly in case of PC the pattern learned from data might not have a DAG with same skeleton and v-structures. Then, it is not clear how to use Chickerings approach. ","category":"page"},{"location":"comments/#On-Meek's-rule-4-1","page":"Developer comments","title":"On Meek's rule 4","text":"","category":"section"},{"location":"comments/#","page":"Developer comments","title":"Developer comments","text":"R4 is only needed in case of background knowledge, i.e. some edge orientations which do not follow from v-structures. For soundness of the rule, it is necessary that v and l are adjacent. For completeness of the Meek rules it suffices to apply it only if there is an undirected edge.","category":"page"},{"location":"#CausalInference.jl-1","page":"Home","title":"CausalInference.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A Julia package for causal inference, graphical models and structure learning. This package contains the PC algorithm pcalg and the extended FCI algorithm, as well as functionality to compute adjustment sets which can be used as control variables to compute causal effects by regression. ","category":"page"},{"location":"#Introduction-1","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The first aim of this package is to provide Julia implementations of popular algorithms for causal structure identification, the PC algorithm, the GES algorithm and the FCI algorithm. The aim of these algorithms is to identify causal relationships in observational data alone, in circumstances where running experiments or A/B tests is impractical or even impossible. While identification of all causal relationships in observational data is not always possible, both algorithms clearly indicate which causal can and which cannot be determined from observational data. Secondly, similarly to DAGitty, covariate adjustment sets for estimating causal effects for example by regression can be computed. ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Causal inference is by no means an easy subject. Readers without any prior exposure to these topics are encouraged to go over the following resources in order to get a basic idea of what's involved in causal inference:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Causal Inference in Statistics: A Primer\nReview of Causal Discovery Methods Based on Graphical Models\nOn Pearl’s Hierarchy and the Foundations of Causal Inference","category":"page"},{"location":"#","page":"Home","title":"Home","text":"There are also tutorials and examples linked in the navigation bar of this package.","category":"page"},{"location":"#Implementation-Details-1","page":"Home","title":"Implementation Details","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The PC algorithm was tested on random DAGs by comparing the result of the PC algorithm using the d-separation oracle with the CPDAG computed with Chickering's DAG->CPDAG conversion algorithm (implemented as dsep and cpdag in this package).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"See the Library for other implemented functionality.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The algorithms use the SimpleGraph and SimpleDiGraph graph representation of the Julia package Graphs. Both types of graphs are represented by sorted adjacency lists (vectors of vectors in the Graphs implementation).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"CPDAGs are just modeled as SimpleDiGraphs, where unoriented edges are represented by a forward and a backward directed edge.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The listing algorithms for adjustment sets are implemented from scratch using an memority efficient iterator protocol to handle large problems.","category":"page"},{"location":"#Performance-1","page":"Home","title":"Performance","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The speed of the PC algorithm is comparable with the C++ code of the R package pcalg.","category":"page"},{"location":"#Plotting-1","page":"Home","title":"Plotting","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Main package provides a text-based output describing all identified edges for PC and FCI algorithm (plot_pc_graph_text and plot_fci_graph_text, respectively).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"In addition, additional plotting backends are supported with lazy code loading orchestrated by Requires.jl. Upon importing of TikzGraphs.jl, additional plotting methods plot_pc_graph_tikz and plot_fci_graph_tikz will be loaded (these are also aliased as plot_pc_graph and plot_fci_graph for backward compatibility). Similarly, upon importing of both GraphRecipes.jl and Plots.jl, additional plotting methods plot_pc_graph_recipes and plot_fci_graph_recipes will be loaded.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"At the time of writing (December 2022), TikzGraphs.jl cannot be installed on ARM-based systems, so GraphRecipes.jl + Plots.jl is the recommended plotting backend in such cases.","category":"page"},{"location":"#Contribution-1","page":"Home","title":"Contribution","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"See issue #1 (Roadmap/Contribution) for questions and coordination of the development.","category":"page"},{"location":"#References-1","page":"Home","title":"References","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"P. Spirtes, C. Glymour, R. Scheines, R. Tillman: Automated search for causal relations: Theory and practice. Heuristics, Probability and Causality: A Tribute to Judea Pearl 2010\nP. Spirtes, C. Glymour, R. Scheines: Causation, Prediction, and Search. MIT Press 2000\nJ. Zhang: On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artificial Intelligence 16-17 (2008), 1873-1896\nT. Richardson, P. Spirtes: Ancestral Graph Markov Models. The Annals of Statistics 30 (2002), 962-1030\nD. M. Chickering: Learning Equivalence Classes of Bayesian-Network Structures. Journal of Machine Learning Research 2 (2002), 445-498.\nD. Colombo, M. H. Maathuis: Order-Independent Constraint-Based Causal Structure Learning. Journal of Machine Learning Research 15 (2014), 3921-3962.\nB. van der Zander, M. Liśkiewicz, J. Textor: Separators and Adjustment Sets in Causal Graphs: Complete Criteria and an Algorithmic Framework. (https://doi.org/10.48550/arXiv.1803.00116)","category":"page"},{"location":"examples/pc_basic_examples/#PC-Algorithm:-Basic-examples-1","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"","category":"section"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"A few example data sets can be useful to illustrate how to work with the PC algorithm and the different independence tests implemented in this package. The examples discussed here are based on the example models discussed in chapter 2 of Judea Pearl's book. The causal model we are going to study can be represented using the following DAG:","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"(Image: True example DAG)","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"We can easily create some sample data that corresponds to the causal structure described by the DAG. For the sake of simplicity, let's create data from a simple linear model that follows the structure defined by the DAG shown above:","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"using CausalInference\nusing TikzGraphs\n# If you have problems with TikzGraphs.jl, \n# try alternatively plotting backend GraphRecipes.jl + Plots.jl\n# and corresponding plotting function `plot_pc_graph_recipes`\n\n# Generate some sample data to use with the PC algorithm\n\nN = 1000 # number of data points\n\n# define simple linear model with added noise\nx = randn(N)\nv = x + randn(N)*0.25\nw = x + randn(N)*0.25\nz = v + w + randn(N)*0.25\ns = z + randn(N)*0.25\n\ndf = (x=x, v=v, w=w, z=z, s=s)","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"With this data ready, we can now see to what extent we can back out the underlying causal structure from the data using the PC algorithm. Under the hood, the PC algorithm uses repeated conditional independence tests to determine the causal relationships between different variables in a given data set. In order to run the PC algorithm on our test data set, we need to specify not only the data set we want to use, but also the conditional independence test alongside a p-value. For now, let's use a simple Gaussian conditional independence test with a p-value of 0.01. ","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"est_g = pcalg(df, 0.01, gausscitest)","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"In order to investigate the output of the PC algorithm, this package also provides a function to easily plot and visually analyse this output.","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"tp = plot_pc_graph_tikz(est_g, [String(k) for k in keys(df)])","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"(Image: Example output of PC algorithm)","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"The first thing that stands out in this plot is that only some edges have arrow marks, while others don't. For example, the edge going from v to z is pointing from v to z, indicating that that v influences z and not the other way around. On the other hand, the edge going from x to w has no arrows on either end, meaning that the direction of causal influence has not been identified and is in fact not identifiable based on the available data alone. Both causal directions, x influencing v and v influencing x, are compatible with the observed data. We can illustrate this directly by switching the direction of influence in the data generating process used above and running the PC algorithm for this new data set:","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"# Generate some additional sample data with different causal relationships\n\nN = 1000 # number of data points\n\n# define simple linear model with added noise\nv = randn(N)\nx = v + randn(N)*0.25\nw = x + randn(N)*0.25\nz = v + w + randn(N)*0.25\ns = z + randn(N)*0.25\n\ndf = (x=x, v=v, w=w, z=z, s=s)\n\nplot_pc_graph_tikz(pcalg(df, 0.01, gausscitest), [String(k) for k in keys(df)])\n","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"(Image: PC results for alternative DAG)","category":"page"},{"location":"examples/pc_basic_examples/#","page":"PC Algorithm: Basic examples","title":"PC Algorithm: Basic examples","text":"We can, however, conclude unequivocally from the available (observational) data alone that w and v are causal for z and z for s. At no point did we have to resort to things like direct interventions, experiments or A/B tests to arrive at these conclusions!","category":"page"}]
}
